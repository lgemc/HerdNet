{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# HerdNet Training with Custom Configuration\n",
    "\n",
    "This notebook contains the same config as this command line (adapt paths as needed):\n",
    "```bash\n",
    "PYTHONPATH=\"$pwd:PYTHONPATH\" uv run tools/train.py \\\n",
    "    train.datasets.train.csv_file=$PWD/data/groundtruth/csv/train_big_size_A_B_E_K_WH_WB_points.csv \\\n",
    "    train.datasets.train.root_dir=$PWD/data/train \\\n",
    "    train.datasets.validate.csv_file=$PWD/data/groundtruth/csv/val_big_size_A_B_E_K_WH_WB_points.csv \\\n",
    "    train.datasets.validate.root_dir=$PWD/data/val \\\n",
    "    train.datasets.anno_type=point\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T00:16:19.660258Z",
     "start_time": "2025-09-14T00:16:16.921383Z"
    }
   },
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import albumentations as A\n",
    "from torch.utils.data import DataLoader\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = os.path.abspath('..')\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "import animaloc\n",
    "from animaloc.models.utils import LossWrapper, load_model\n",
    "from animaloc.eval import Evaluator, PointsMetrics, Stitcher, BoxesMetrics, ImageLevelMetrics\n",
    "from animaloc.utils.seed import set_seed\n",
    "from animaloc.utils.useful_funcs import current_date\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name()}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0+cu128\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA GeForce RTX 4050 Laptop GPU\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T00:16:19.698038Z",
     "start_time": "2025-09-14T00:16:19.684993Z"
    }
   },
   "source": "# Load base configuration using the main config (like train.py does)\nmain_cfg_path = os.path.join(project_root, 'configs/train/herdnet.yaml')\ncfg = OmegaConf.load(main_cfg_path)\n\n# Override with custom paths (matching the command line args)\ncfg.datasets.train.csv_file = os.path.join(project_root, 'data/groundtruth/csv/train_big_size_A_B_E_K_WH_WB_points.csv')\ncfg.datasets.train.root_dir = os.path.join(project_root, 'data/train')\ncfg.datasets.validate.csv_file = os.path.join(project_root, 'data/groundtruth/csv/val_big_size_A_B_E_K_WH_WB_points.csv')\ncfg.datasets.validate.root_dir = os.path.join(project_root, 'data/val')\ncfg.datasets.anno_type = 'point'\n\n# Update class definitions for 6 classes based on your data\ncfg.datasets.num_classes = 7  # Including background class\ncfg.datasets.class_def = {\n    1: 'buffalo',\n    2: 'elephant', \n    3: 'kob',\n    4: 'warthog',\n    5: 'waterbuck',\n    6: 'other'\n}\n\n# Update loss weights for 6 classes + background\ncfg.losses.CrossEntropyLoss.kwargs.weight = [0.1, 5., 15., 1., 5., 5., 1.]\n\n# Resolve interpolations - fix the incorrect interpolation keys in the config\n# The config uses ${train.datasets.num_classes} but should use ${datasets.num_classes}\n# We need to manually resolve these until the config is fixed\nif 'end_transforms' in cfg.datasets.train and cfg.datasets.train.end_transforms is not None:\n    if 'MultiTransformsWrapper' in cfg.datasets.train.end_transforms:\n        if 'FIDT' in cfg.datasets.train.end_transforms.MultiTransformsWrapper:\n            cfg.datasets.train.end_transforms.MultiTransformsWrapper.FIDT.num_classes = cfg.datasets.num_classes\n            cfg.datasets.train.end_transforms.MultiTransformsWrapper.FIDT.down_ratio = cfg.model.kwargs.down_ratio\n        if 'PointsToMask' in cfg.datasets.train.end_transforms.MultiTransformsWrapper:\n            cfg.datasets.train.end_transforms.MultiTransformsWrapper.PointsToMask.num_classes = cfg.datasets.num_classes\n\nif 'end_transforms' in cfg.datasets.validate and cfg.datasets.validate.end_transforms is not None:\n    if 'DownSample' in cfg.datasets.validate.end_transforms:\n        cfg.datasets.validate.end_transforms.DownSample.down_ratio = cfg.model.kwargs.down_ratio\n        cfg.datasets.validate.end_transforms.DownSample.anno_type = cfg.datasets.anno_type\n\n# Also resolve interpolations in training_settings.stitcher\nif cfg.training_settings.stitcher is not None:\n    cfg.training_settings.stitcher.kwargs.down_ratio = cfg.model.kwargs.down_ratio\n\nprint(\"Configuration loaded and customized\")\nprint(f\"Training CSV: {cfg.datasets.train.csv_file}\")\nprint(f\"Training root: {cfg.datasets.train.root_dir}\")\nprint(f\"Validation CSV: {cfg.datasets.validate.csv_file}\")\nprint(f\"Validation root: {cfg.datasets.validate.root_dir}\")\nprint(f\"Number of classes: {cfg.datasets.num_classes}\")\nprint(f\"Annotation type: {cfg.datasets.anno_type}\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded and customized\n",
      "Training CSV: /home/lmanrique/Do/HerdNet/data/groundtruth/csv/train_big_size_A_B_E_K_WH_WB_points.csv\n",
      "Training root: /home/lmanrique/Do/HerdNet/data/train\n",
      "Validation CSV: /home/lmanrique/Do/HerdNet/data/groundtruth/csv/val_big_size_A_B_E_K_WH_WB_points.csv\n",
      "Validation root: /home/lmanrique/Do/HerdNet/data/val\n",
      "Number of classes: 7\n",
      "Annotation type: point\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "\n",
    "These functions replicate the helper functions from the train.py script"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T00:16:19.796636Z",
     "start_time": "2025-09-14T00:16:19.790927Z"
    }
   },
   "source": [
    "def _load_albu_transforms(tr_cfg: dict) -> list:\n",
    "    transforms = []\n",
    "    for name, kwargs in tr_cfg.items():\n",
    "        transforms.append(A.__dict__[name](**kwargs))\n",
    "    return transforms\n",
    "\n",
    "def _load_end_transforms(tr_cfg):\n",
    "    if tr_cfg is not None:\n",
    "        transforms = []\n",
    "        for name, kwargs in tr_cfg.items():\n",
    "            if name == 'MultiTransformsWrapper':\n",
    "                tr_list = []\n",
    "                for n, k in kwargs.items():\n",
    "                    tr_list.append(animaloc.data.transforms.__dict__[n](**k))\n",
    "                transforms.append(animaloc.data.transforms.__dict__[name](tr_list))\n",
    "            else:\n",
    "                transforms.append(animaloc.data.transforms.__dict__[name](**kwargs))\n",
    "        return transforms\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def _get_collate_fn(cfg):\n",
    "    fn = cfg.datasets.collate_fn\n",
    "    if fn is not None:\n",
    "        fn = animaloc.data.batch_utils.__dict__[fn]\n",
    "    return fn\n",
    "\n",
    "def _build_model(cfg) -> torch.nn.Module:\n",
    "    name = cfg.model.name\n",
    "    from_torchvision = cfg.model.from_torchvision\n",
    "    \n",
    "    if from_torchvision:\n",
    "        import torchvision\n",
    "        assert name in torchvision.models.__dict__.keys(), f\"'{name}' unfound in torchvision's models\"\n",
    "        model = torchvision.models.__dict__[name]\n",
    "    else:\n",
    "        assert name in animaloc.models.__dict__.keys(), f\"'{name}' class unfound, make sure you have included the class in the models list\"\n",
    "        model = animaloc.models.__dict__[name]\n",
    "    \n",
    "    kwargs = dict(cfg.model.kwargs)\n",
    "    for k in ['num_classes']:\n",
    "        kwargs.pop(k, None)\n",
    "    \n",
    "    model = model(**kwargs, num_classes=cfg.datasets.num_classes)\n",
    "    return model\n",
    "\n",
    "def _load_losses(cfg) -> tuple:\n",
    "    criterions = []\n",
    "    if cfg.losses is not None:\n",
    "        for loss, args in cfg.losses.items():\n",
    "            kwargs = {}\n",
    "            if 'kwargs' in args.keys():\n",
    "                kwargs = dict(args.kwargs)\n",
    "                \n",
    "                if 'weights' in kwargs.keys():\n",
    "                    kwargs['weights'] = torch.Tensor(kwargs['weights'])\n",
    "                elif 'weight' in kwargs.keys():\n",
    "                    kwargs['weight'] = torch.Tensor(kwargs['weight']).to(torch.device(cfg.device_name))\n",
    "            \n",
    "            crit_dict = {}\n",
    "            if args.from_torch:\n",
    "                crit_dict.update({'loss': torch.nn.__dict__[loss](**kwargs)})\n",
    "            else:\n",
    "                crit_dict.update({'loss': animaloc.train.losses.__dict__[loss](**kwargs)})\n",
    "            \n",
    "            crit_dict.update({\n",
    "                'idx': args.output_idx,\n",
    "                'idy': args.target_idx,\n",
    "                'lambda': args.lambda_const,\n",
    "                'name': args.print_name\n",
    "            })\n",
    "            \n",
    "            criterions.append(crit_dict)\n",
    "    \n",
    "    return criterions\n",
    "\n",
    "def _define_stitcher(model, cfg):\n",
    "    kwargs = dict(cfg.training_settings.stitcher.kwargs)\n",
    "    for k in ['model', 'size', 'device_name']:\n",
    "        kwargs.pop(k, None)\n",
    "    \n",
    "    stitcher = animaloc.eval.stitchers.__dict__[cfg.training_settings.stitcher.name](\n",
    "        model=model,\n",
    "        size=cfg.datasets.img_size,\n",
    "        **kwargs,\n",
    "        device_name=cfg.device_name\n",
    "    )\n",
    "    \n",
    "    return stitcher\n",
    "\n",
    "def _define_evaluator(model, dataloader, cfg):\n",
    "    name = cfg.training_settings.evaluator.name\n",
    "    anno_type = cfg.datasets.anno_type\n",
    "    \n",
    "    assert name in animaloc.eval.evaluators.__dict__.keys(), f\"'{name}' class unfound\"\n",
    "    \n",
    "    if anno_type == 'point':\n",
    "        metrics = PointsMetrics(\n",
    "            radius=cfg.training_settings.evaluator.threshold,\n",
    "            num_classes=cfg.datasets.num_classes\n",
    "        )\n",
    "    elif anno_type == 'bbox':\n",
    "        metrics = BoxesMetrics(\n",
    "            iou=cfg.training_settings.evaluator.threshold,\n",
    "            num_classes=cfg.datasets.num_classes\n",
    "        )\n",
    "    elif anno_type == 'image':\n",
    "        metrics = ImageLevelMetrics(\n",
    "            num_classes=cfg.datasets.num_classes\n",
    "        )\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    stitcher = None\n",
    "    if cfg.training_settings.stitcher is not None:\n",
    "        stitcher = _define_stitcher(model, cfg)\n",
    "    \n",
    "    kwargs = dict(cfg.training_settings.evaluator.kwargs)\n",
    "    for k in ['model', 'dataloader', 'metrics', 'device_name', 'stitcher', 'header', 'vizual_fn']:\n",
    "        kwargs.pop(k, None)\n",
    "    \n",
    "    vizual_fn = None\n",
    "    if cfg.training_settings.vizual_fn is not None:\n",
    "        vizual_fn = animaloc.vizual.plots.__dict__[cfg.training_settings.vizual_fn]\n",
    "    \n",
    "    evaluator = animaloc.eval.evaluators.__dict__[name](\n",
    "        model=model,\n",
    "        dataloader=dataloader,\n",
    "        metrics=metrics,\n",
    "        device_name=cfg.device_name,\n",
    "        stitcher=stitcher,\n",
    "        header='[TEST]',\n",
    "        vizual_fn=vizual_fn,\n",
    "        **kwargs\n",
    "    )\n",
    "    \n",
    "    return evaluator\n",
    "\n",
    "print(\"Utility functions defined\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utility functions defined\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Training"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T00:16:19.838501Z",
     "start_time": "2025-09-14T00:16:19.835640Z"
    }
   },
   "source": [
    "# Set the seed\n",
    "print(f'Setting the seed to {cfg.seed}')\n",
    "set_seed(cfg.seed)\n",
    "\n",
    "# Setup device\n",
    "device = torch.device(cfg.device_name)\n",
    "print(f\"Using device: {device}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the seed to 1\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Datasets and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T00:16:19.978253Z",
     "start_time": "2025-09-14T00:16:19.884179Z"
    }
   },
   "source": [
    "print('Building datasets ...')\n",
    "\n",
    "# Training dataset\n",
    "train_args = cfg.datasets.train\n",
    "val_args = cfg.datasets.validate\n",
    "\n",
    "train_df = pd.read_csv(train_args.csv_file)\n",
    "print(f\"Training data loaded: {len(train_df)} samples\")\n",
    "print(f\"Training CSV columns: {list(train_df.columns)}\")\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "\n",
    "train_dataset = animaloc.datasets.__dict__[train_args.name](\n",
    "    csv_file=train_df,\n",
    "    root_dir=train_args.root_dir,\n",
    "    albu_transforms=_load_albu_transforms(train_args.albu_transforms),\n",
    "    end_transforms=_load_end_transforms(train_args.end_transforms)\n",
    ")\n",
    "\n",
    "train_dl_kwargs = dict(\n",
    "    batch_size=cfg.training_settings.batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=_get_collate_fn(cfg)\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, **train_dl_kwargs)\n",
    "print(f\"Training dataloader created with batch size: {cfg.training_settings.batch_size}\")\n",
    "\n",
    "# Validation dataset\n",
    "val_dataloader = None\n",
    "if val_args is not None:\n",
    "    val_df = pd.read_csv(val_args.csv_file)\n",
    "    print(f\"Validation data loaded: {len(val_df)} samples\")\n",
    "    print(f\"Validation data shape: {val_df.shape}\")\n",
    "    \n",
    "    val_dataset = animaloc.datasets.__dict__[val_args.name](\n",
    "        csv_file=val_df,\n",
    "        root_dir=val_args.root_dir,\n",
    "        albu_transforms=_load_albu_transforms(val_args.albu_transforms),\n",
    "        end_transforms=_load_end_transforms(val_args.end_transforms)\n",
    "    )\n",
    "    \n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False, collate_fn=_get_collate_fn(cfg))\n",
    "    print(\"Validation dataloader created\")\n",
    "\n",
    "print(f\"Dataset setup complete!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building datasets ...\n",
      "Training data loaded: 6962 samples\n",
      "Training CSV columns: ['images', 'x', 'y', 'labels']\n",
      "Training data shape: (6962, 4)\n",
      "Training dataloader created with batch size: 4\n",
      "Validation data loaded: 978 samples\n",
      "Validation data shape: (978, 4)\n",
      "Validation dataloader created\n",
      "Dataset setup complete!\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Weights & Biases Logging"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T00:16:21.690248Z",
     "start_time": "2025-09-14T00:16:19.985842Z"
    }
   },
   "source": [
    "print('Connecting to Weights & Biases ...')\n",
    "settings = cfg.training_settings\n",
    "losses = cfg.losses\n",
    "if losses is not None:\n",
    "    losses = list(cfg.losses.keys())\n",
    "\n",
    "wandb.init(\n",
    "    project=cfg.wandb_project,\n",
    "    entity=cfg.wandb_entity,\n",
    "    config=dict(\n",
    "        batch_size=settings.batch_size,\n",
    "        optimizer=settings.optimizer,\n",
    "        lr=settings.lr,\n",
    "        weight_decay=settings.weight_decay,\n",
    "        warmup_iters=settings.warmup_iters,\n",
    "        epochs=settings.epochs,\n",
    "        losses=losses,\n",
    "        seed=cfg.seed,\n",
    "        data_augmentation=list(cfg.datasets.train.albu_transforms.keys()),\n",
    "        input_size=cfg.datasets.img_size,\n",
    "        **cfg.model.kwargs\n",
    "    )\n",
    ")\n",
    "\n",
    "date = current_date()\n",
    "wandb.run.name = f'{date}_' + cfg.wandb_run + f'_RUN_{wandb.run.id}'\n",
    "print(f\"W&B run name: {wandb.run.name}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to Weights & Biases ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mluis-manrique-car\u001B[0m (\u001B[33mluis-manrique-car-camera-traps\u001B[0m) to \u001B[32mhttps://api.wandb.ai\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.21.4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/lmanrique/Do/HerdNet/notebooks/wandb/run-20250913_191620-ax140pht</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/luis-manrique-car-camera-traps/camera-traps/runs/ax140pht' target=\"_blank\">fearless-wildflower-14</a></strong> to <a href='https://wandb.ai/luis-manrique-car-camera-traps/camera-traps' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/luis-manrique-car-camera-traps/camera-traps' target=\"_blank\">https://wandb.ai/luis-manrique-car-camera-traps/camera-traps</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/luis-manrique-car-camera-traps/camera-traps/runs/ax140pht' target=\"_blank\">https://wandb.ai/luis-manrique-car-camera-traps/camera-traps/runs/ax140pht</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B run name: 20250913_camera-traps-train_RUN_ax140pht\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model and Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T00:16:21.940679Z",
     "start_time": "2025-09-14T00:16:21.709760Z"
    }
   },
   "source": [
    "print('Building the model ...')\n",
    "model = _build_model(cfg)\n",
    "print(f\"Model created: {cfg.model.name}\")\n",
    "\n",
    "print('Preparing for training ...')\n",
    "criterions = _load_losses(cfg)\n",
    "model = LossWrapper(model, criterions).to(device)\n",
    "print(f\"Model wrapped with losses and moved to {device}\")\n",
    "\n",
    "# Load pretrained weights if specified\n",
    "if cfg.model.load_from is not None:\n",
    "    model = load_model(model, cfg.model.load_from)\n",
    "    print(f\"Loaded pretrained weights from {cfg.model.load_from}\")\n",
    "    \n",
    "    if 'HerdNet' in cfg.model.name:\n",
    "        if cfg.model.freeze is not None:\n",
    "            model.model.freeze(layers=list(cfg.model.freeze))\n",
    "            print(f\"Layers {list(cfg.model.freeze)} frozen\")\n",
    "\n",
    "# Setup optimizer\n",
    "if cfg.training_settings.optimizer == 'adam':\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=cfg.training_settings.lr,\n",
    "        weight_decay=cfg.training_settings.weight_decay\n",
    "    )\n",
    "else:\n",
    "    optimizer = torch.optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=cfg.training_settings.lr,\n",
    "        weight_decay=cfg.training_settings.weight_decay\n",
    "    )\n",
    "\n",
    "print(f\"Optimizer: {cfg.training_settings.optimizer}\")\n",
    "print(f\"Learning rate: {cfg.training_settings.lr}\")\n",
    "print(f\"Weight decay: {cfg.training_settings.weight_decay}\")\n",
    "\n",
    "# Watch the model's gradients during training\n",
    "wandb.watch(model)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the model ...\n",
      "Model created: HerdNet\n",
      "Preparing for training ...\n",
      "Model wrapped with losses and moved to cuda\n",
      "Optimizer: adam\n",
      "Learning rate: 0.0001\n",
      "Weight decay: 0.0005\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T00:16:21.952288Z",
     "start_time": "2025-09-14T00:16:21.947596Z"
    }
   },
   "source": [
    "evaluator = None\n",
    "validate_on = 'recall'\n",
    "select = 'min'\n",
    "\n",
    "if cfg.training_settings.evaluator is not None:\n",
    "    assert val_dataloader is not None, 'A validation dataset must be defined to build an evaluator'\n",
    "    \n",
    "    evaluator = _define_evaluator(model, val_dataloader, cfg)\n",
    "    select = cfg.training_settings.evaluator.select_mode\n",
    "    validate_on = cfg.training_settings.evaluator.validate_on\n",
    "    print(f\"Evaluator created - selecting {select} {validate_on}\")\n",
    "else:\n",
    "    print(\"No evaluator configured\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluator created - selecting max f1_score\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Trainer and Start Training"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T00:16:22.005757Z",
     "start_time": "2025-09-14T00:16:22.000966Z"
    }
   },
   "source": [
    "# Setup auto learning rate scheduler\n",
    "auto_lr = cfg.training_settings.auto_lr\n",
    "if auto_lr:\n",
    "    auto_lr = dict(cfg.training_settings.auto_lr)\n",
    "    print(f\"Auto LR scheduler enabled: {auto_lr}\")\n",
    "\n",
    "# Setup visualization function\n",
    "vizual_fn = None\n",
    "if cfg.training_settings.vizual_fn is not None:\n",
    "    vizual_fn = animaloc.vizual.plots.__dict__[cfg.training_settings.vizual_fn]\n",
    "\n",
    "# Create trainer\n",
    "trainer = animaloc.train.trainers.__dict__[cfg.training_settings.trainer](\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=cfg.training_settings.epochs,\n",
    "    auto_lr=auto_lr,\n",
    "    val_dataloader=val_dataloader,\n",
    "    evaluator=evaluator,\n",
    "    device_name=cfg.device_name,\n",
    "    vizual_fn=vizual_fn,\n",
    "    work_dir=None,\n",
    "    print_freq=cfg.training_settings.print_freq,\n",
    "    valid_freq=cfg.training_settings.valid_freq,\n",
    ")\n",
    "\n",
    "print(f\"Trainer created: {cfg.training_settings.trainer}\")\n",
    "print(f\"Training for {cfg.training_settings.epochs} epochs\")\n",
    "print(f\"Validation frequency: every {cfg.training_settings.valid_freq} epoch(s)\")\n",
    "print(f\"Print frequency: every {cfg.training_settings.print_freq} iterations\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auto LR scheduler enabled: {'mode': 'max', 'patience': 10, 'threshold': 0.0001, 'threshold_mode': 'rel', 'cooldown': 10, 'min_lr': 1e-06}\n",
      "Trainer created: Trainer\n",
      "Training for 100 epochs\n",
      "Validation frequency: every 1 epoch(s)\n",
      "Print frequency: every 100 iterations\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T00:16:50.449407Z",
     "start_time": "2025-09-14T00:16:22.105607Z"
    }
   },
   "source": [
    "# Start training\n",
    "if cfg.model.resume_from is not None:\n",
    "    print(f'Resuming training from \\'{cfg.model.resume_from}\\' ...')\n",
    "    trainer.resume(\n",
    "        pth_path=cfg.model.resume_from,\n",
    "        select=select,\n",
    "        validate_on=validate_on,\n",
    "        load_optim=True,\n",
    "        wandb_flag=True\n",
    "    )\n",
    "else:\n",
    "    print('Starting training ...')\n",
    "    trainer.start(\n",
    "        cfg.training_settings.warmup_iters,\n",
    "        select=select,\n",
    "        validate_on=validate_on,\n",
    "        wandb_flag=True\n",
    "    )"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training ...\n",
      "[TRAINING] - Epoch: [1] [  1/232] eta: 0:04:22 lr: 0.000002 loss: 15318.1357 (15318.1357) focal_loss: 15316.1514 (15316.1514) ce_loss: 1.9848 (1.9848) time: 1.1331 data: 0.5681 max mem: 2104\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[10]\u001B[39m\u001B[32m, line 13\u001B[39m\n\u001B[32m     11\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     12\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33m'\u001B[39m\u001B[33mStarting training ...\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m13\u001B[39m     \u001B[43mtrainer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstart\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     14\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcfg\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtraining_settings\u001B[49m\u001B[43m.\u001B[49m\u001B[43mwarmup_iters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     15\u001B[39m \u001B[43m        \u001B[49m\u001B[43mselect\u001B[49m\u001B[43m=\u001B[49m\u001B[43mselect\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     16\u001B[39m \u001B[43m        \u001B[49m\u001B[43mvalidate_on\u001B[49m\u001B[43m=\u001B[49m\u001B[43mvalidate_on\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     17\u001B[39m \u001B[43m        \u001B[49m\u001B[43mwandb_flag\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\n\u001B[32m     18\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Do/HerdNet/animaloc/train/trainers.py:273\u001B[39m, in \u001B[36mTrainer.start\u001B[39m\u001B[34m(self, warmup_iters, checkpoints, select, validate_on, wandb_flag)\u001B[39m\n\u001B[32m    268\u001B[39m     wandb.log({\u001B[33m'\u001B[39m\u001B[33mlr\u001B[39m\u001B[33m'\u001B[39m: \u001B[38;5;28mself\u001B[39m.optimizer.param_groups[\u001B[32m0\u001B[39m][\u001B[33m\"\u001B[39m\u001B[33mlr\u001B[39m\u001B[33m\"\u001B[39m]})\n\u001B[32m    270\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[32m1\u001B[39m,\u001B[38;5;28mself\u001B[39m.epochs + \u001B[32m1\u001B[39m):\n\u001B[32m    271\u001B[39m \n\u001B[32m    272\u001B[39m     \u001B[38;5;66;03m# training\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m273\u001B[39m     train_output = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_train\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepoch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwarmup_iters\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwandb_flag\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    274\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m wandb_flag:\n\u001B[32m    275\u001B[39m         wandb.log({\u001B[33m'\u001B[39m\u001B[33mtrain_loss\u001B[39m\u001B[33m'\u001B[39m: train_output, \u001B[33m'\u001B[39m\u001B[33mepoch\u001B[39m\u001B[33m'\u001B[39m: epoch})\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Do/HerdNet/animaloc/train/trainers.py:520\u001B[39m, in \u001B[36mTrainer._train\u001B[39m\u001B[34m(self, epoch, warmup_iters, wandb_flag)\u001B[39m\n\u001B[32m    516\u001B[39m images, targets = \u001B[38;5;28mself\u001B[39m.prepare_data(images, targets)\n\u001B[32m    518\u001B[39m \u001B[38;5;28mself\u001B[39m.optimizer.zero_grad()\n\u001B[32m--> \u001B[39m\u001B[32m520\u001B[39m loss_dict = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtargets\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    522\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m wandb_flag:\n\u001B[32m    523\u001B[39m     wandb.log(loss_dict)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Do/HerdNet/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1771\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1772\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Do/HerdNet/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1779\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1780\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1781\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1782\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1783\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1784\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1786\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1787\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Do/HerdNet/animaloc/models/utils.py:132\u001B[39m, in \u001B[36mLossWrapper.forward\u001B[39m\u001B[34m(self, x, target)\u001B[39m\n\u001B[32m    130\u001B[39m         reg = dic[\u001B[33m'\u001B[39m\u001B[33mlambda\u001B[39m\u001B[33m'\u001B[39m]\n\u001B[32m    131\u001B[39m         loss_module = dic[\u001B[33m'\u001B[39m\u001B[33mloss\u001B[39m\u001B[33m'\u001B[39m]\n\u001B[32m--> \u001B[39m\u001B[32m132\u001B[39m         loss = \u001B[43mloss_module\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutput_used\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m[\u001B[49m\u001B[43mj\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    133\u001B[39m         output_dict.update({dic[\u001B[33m'\u001B[39m\u001B[33mname\u001B[39m\u001B[33m'\u001B[39m] : reg * loss})\n\u001B[32m    135\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.output_mode == \u001B[33m'\u001B[39m\u001B[33mmodule\u001B[39m\u001B[33m'\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Do/HerdNet/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1771\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1772\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Do/HerdNet/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1779\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1780\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1781\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1782\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1783\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1784\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1786\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1787\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Do/HerdNet/animaloc/train/losses/focal.py:77\u001B[39m, in \u001B[36mFocalLoss.forward\u001B[39m\u001B[34m(self, output, target)\u001B[39m\n\u001B[32m     67\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, output: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n\u001B[32m     68\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m'''\u001B[39;00m\n\u001B[32m     69\u001B[39m \u001B[33;03m    Args:\u001B[39;00m\n\u001B[32m     70\u001B[39m \u001B[33;03m        output (torch.Tensor): [B,C,H,W]\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m     74\u001B[39m \u001B[33;03m        torch.Tensor\u001B[39;00m\n\u001B[32m     75\u001B[39m \u001B[33;03m    '''\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m77\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_neg_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Do/HerdNet/animaloc/train/losses/focal.py:116\u001B[39m, in \u001B[36mFocalLoss._neg_loss\u001B[39m\u001B[34m(self, output, target)\u001B[39m\n\u001B[32m    114\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m b \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(B):\n\u001B[32m    115\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(C):\n\u001B[32m--> \u001B[39m\u001B[32m116\u001B[39m         density = \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mneg_loss\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    117\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.density_weight == \u001B[33m'\u001B[39m\u001B[33mlinear\u001B[39m\u001B[33m'\u001B[39m:\n\u001B[32m    118\u001B[39m             density = num_pos[b][c]\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Training: Add Metadata to Model Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add information in .pth files\n",
    "print(\"Adding metadata to model files...\")\n",
    "for pth_name in ['best_model.pth', 'latest_model.pth']:\n",
    "    path = os.path.join(os.getcwd(), pth_name)\n",
    "    if os.path.exists(path):\n",
    "        pth_file = torch.load(path)\n",
    "        norm_trans = _load_albu_transforms(train_args.albu_transforms)[-1]\n",
    "        pth_file['classes'] = dict(cfg.datasets.class_def)\n",
    "        pth_file['mean'] = list(norm_trans.mean)\n",
    "        pth_file['std'] = list(norm_trans.std)\n",
    "        torch.save(pth_file, path)\n",
    "        print(f\"Updated {pth_name} with metadata\")\n",
    "    else:\n",
    "        print(f\"Warning: {pth_name} not found\")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Display Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some training information\n",
    "print(\"\\n=== Training Summary ===\")\n",
    "print(f\"Model: {cfg.model.name}\")\n",
    "print(f\"Number of classes: {cfg.datasets.num_classes}\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "if val_dataloader:\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Batch size: {cfg.training_settings.batch_size}\")\n",
    "print(f\"Learning rate: {cfg.training_settings.lr}\")\n",
    "print(f\"Epochs: {cfg.training_settings.epochs}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Show W&B run URL\n",
    "print(f\"\\nW&B Run: {wandb.run.url}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

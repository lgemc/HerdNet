{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HerdNet Model Evaluation\n",
    "\n",
    "This notebook evaluates the latest trained HerdNet model on the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T11:58:35.771056Z",
     "start_time": "2025-09-14T11:58:35.768317Z"
    }
   },
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import albumentations as A\n",
    "from torch.utils.data import DataLoader\n",
    "from animaloc.datasets import CSVDataset\n",
    "from animaloc.data.transforms import DownSample\n",
    "from animaloc.models import HerdNet, load_model\n",
    "from animaloc.eval import PointsMetrics, HerdNetStitcher, HerdNetEvaluator\n",
    "from animaloc.utils.seed import set_seed\n",
    "from animaloc.utils.useful_funcs import mkdir\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed(1)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T11:58:36.587681Z",
     "start_time": "2025-09-14T11:58:36.583958Z"
    }
   },
   "source": [
    "# Model configuration (matching training config)\n",
    "num_classes = 7\n",
    "down_ratio = 2\n",
    "patch_size = 512\n",
    "\n",
    "# Data paths\n",
    "test_csv = '/home/lmanrique/Do/HerdNet/data/groundtruth/csv/test_big_size_A_B_E_K_WH_WB_points.csv'\n",
    "test_root = '/home/lmanrique/Do/HerdNet/data/test'\n",
    "\n",
    "# Find the latest model\n",
    "output_dirs = glob.glob('/home/lmanrique/Do/HerdNet/outputs/2025-09-13/*/')\n",
    "output_dirs.sort(key=lambda x: os.path.basename(x.rstrip('/')))\n",
    "latest_dir = output_dirs[-1]\n",
    "\n",
    "# Look for best_model.pth, fallback to latest_model.pth\n",
    "best_model_path = os.path.join(latest_dir, 'best_model.pth')\n",
    "latest_model_path = os.path.join(latest_dir, 'latest_model.pth')\n",
    "\n",
    "if os.path.exists(best_model_path):\n",
    "  model_path = best_model_path\n",
    "  print(f\"Using best model: {model_path}\")\n",
    "elif os.path.exists(latest_model_path):\n",
    "  model_path = latest_model_path\n",
    "  print(f\"Using latest model: {model_path}\")\n",
    "else:\n",
    "  raise FileNotFoundError(f\"No model found in {latest_dir}\")\n",
    "\n",
    "# Output directory for evaluation results\n",
    "eval_dir = '/home/lmanrique/Do/HerdNet/evaluation_output'\n",
    "mkdir(eval_dir)\n",
    "\n",
    "print(f\"Test CSV: {test_csv}\")\n",
    "print(f\"Test images: {test_root}\")\n",
    "print(f\"Model path: {model_path}\")\n",
    "print(f\"Output directory: {eval_dir}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using latest model: /home/lmanrique/Do/HerdNet/outputs/2025-09-13/22-52-06/latest_model.pth\n",
      "Test CSV: /home/lmanrique/Do/HerdNet/data/groundtruth/csv/test_big_size_A_B_E_K_WH_WB_points.csv\n",
      "Test images: /home/lmanrique/Do/HerdNet/data/test\n",
      "Model path: /home/lmanrique/Do/HerdNet/outputs/2025-09-13/22-52-06/latest_model.pth\n",
      "Output directory: /home/lmanrique/Do/HerdNet/evaluation_output\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T11:58:37.962109Z",
     "start_time": "2025-09-14T11:58:37.905421Z"
    }
   },
   "source": [
    "# Test dataset with normalization and downsampling transforms\n",
    "test_dataset = CSVDataset(\n",
    "    csv_file=test_csv,\n",
    "    root_dir=test_root,\n",
    "    albu_transforms=[\n",
    "        A.Resize(height=patch_size, width=patch_size, p=1.0),\n",
    "        A.Normalize(p=1.0)\n",
    "    ],\n",
    "    end_transforms=[DownSample(down_ratio=down_ratio, anno_type='point')]\n",
    ")\n",
    "\n",
    "# Test dataloader\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=test_dataset, \n",
    "    batch_size=1, \n",
    "    shuffle=False,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset size: 258\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T11:58:39.507996Z",
     "start_time": "2025-09-14T11:58:39.258902Z"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "# Initialize HerdNet model\n",
    "herdnet = HerdNet(\n",
    "    num_classes=num_classes,\n",
    "    down_ratio=down_ratio,\n",
    "    num_layers=34,\n",
    "    pretrained=False,  # Set to False since we're loading trained weights\n",
    "    head_conv=64\n",
    ").cuda()\n",
    "\n",
    "# Load trained model weights with custom handling for LossWrapper\n",
    "checkpoint = torch.load(model_path, map_location='cuda')\n",
    "\n",
    "# Check if the model was saved with LossWrapper (keys prefixed with 'model.')\n",
    "if 'model_state_dict' in checkpoint:\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "else:\n",
    "    state_dict = checkpoint\n",
    "\n",
    "# Remove 'model.' prefix if present\n",
    "if any(key.startswith('model.') for key in state_dict.keys()):\n",
    "    new_state_dict = {}\n",
    "    for key, value in state_dict.items():\n",
    "        if key.startswith('model.'):\n",
    "            new_key = key[6:]  # Remove 'model.' prefix\n",
    "            new_state_dict[new_key] = value\n",
    "        else:\n",
    "            new_state_dict[key] = value\n",
    "    state_dict = new_state_dict\n",
    "\n",
    "# Load the corrected state dict\n",
    "herdnet.load_state_dict(state_dict, strict=False)  # Use strict=False to handle minor mismatches\n",
    "print(f\"Model loaded successfully from {model_path}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from /home/lmanrique/Do/HerdNet/outputs/2025-09-13/22-52-06/latest_model.pth\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Evaluation Components"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T11:58:41.177298Z",
     "start_time": "2025-09-14T11:58:41.172663Z"
    }
   },
   "source": [
    "# Evaluation metrics (radius=5 to match config threshold)\n",
    "metrics = PointsMetrics(radius=5, num_classes=num_classes)\n",
    "\n",
    "# Stitcher for handling full-size images\n",
    "stitcher = HerdNetStitcher(\n",
    "    model=herdnet,\n",
    "    size=(patch_size, patch_size),\n",
    "    overlap=0,  # No overlap as per config\n",
    "    down_ratio=down_ratio,\n",
    "    reduction='mean'\n",
    ")\n",
    "\n",
    "# Evaluator\n",
    "evaluator = HerdNetEvaluator(\n",
    "    model=herdnet,\n",
    "    dataloader=test_dataloader,\n",
    "    metrics=metrics,\n",
    "    stitcher=stitcher,\n",
    "    work_dir=eval_dir,\n",
    "    header='test_evaluation'\n",
    ")\n",
    "\n",
    "print(\"Evaluation components initialized\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation components initialized\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T11:58:44.409824Z",
     "start_time": "2025-09-14T11:58:43.706058Z"
    }
   },
   "source": [
    "# Run evaluation and get F1 score\n",
    "print(\"Starting evaluation...\")\n",
    "test_f1_score = evaluator.evaluate(returns='f1_score')\n",
    "\n",
    "# Print results\n",
    "print(f\"\\n=== EVALUATION RESULTS ===\")\n",
    "print(f\"Global F1 Score: {test_f1_score * 100:.2f}%\")\n",
    "print(f\"Model: {os.path.basename(model_path)}\")\n",
    "print(f\"Test dataset: {len(test_dataset)} images\")\n",
    "print(f\"Results saved to: {eval_dir}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation...\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mIndexError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[10]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# Run evaluation and get F1 score\u001B[39;00m\n\u001B[32m      2\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mStarting evaluation...\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m test_f1_score = \u001B[43mevaluator\u001B[49m\u001B[43m.\u001B[49m\u001B[43mevaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreturns\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mf1_score\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m      5\u001B[39m \u001B[38;5;66;03m# Print results\u001B[39;00m\n\u001B[32m      6\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m=== EVALUATION RESULTS ===\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Do/HerdNet/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001B[39m, in \u001B[36mcontext_decorator.<locals>.decorate_context\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    117\u001B[39m \u001B[38;5;129m@functools\u001B[39m.wraps(func)\n\u001B[32m    118\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdecorate_context\u001B[39m(*args, **kwargs):\n\u001B[32m    119\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[32m--> \u001B[39m\u001B[32m120\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Do/HerdNet/animaloc/eval/evaluators.py:191\u001B[39m, in \u001B[36mEvaluator.evaluate\u001B[39m\u001B[34m(self, returns, wandb_flag, viz, log_meters)\u001B[39m\n\u001B[32m    188\u001B[39m images, targets = \u001B[38;5;28mself\u001B[39m.prepare_data(images, targets)\n\u001B[32m    190\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.stitcher \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m191\u001B[39m     output = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mstitcher\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    192\u001B[39m     output = \u001B[38;5;28mself\u001B[39m.post_stitcher(output)\n\u001B[32m    193\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    194\u001B[39m     \u001B[38;5;66;03m# output, _ = self.model(images, targets)  \u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Do/HerdNet/animaloc/eval/stitchers.py:118\u001B[39m, in \u001B[36mStitcher.__call__\u001B[39m\u001B[34m(self, image)\u001B[39m\n\u001B[32m    115\u001B[39m patches = \u001B[38;5;28mself\u001B[39m.make_patches()\n\u001B[32m    117\u001B[39m \u001B[38;5;66;03m# step 2 - inference to get maps\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m118\u001B[39m det_maps = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_inference\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpatches\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    120\u001B[39m \u001B[38;5;66;03m# step 3 - patch the maps into initial coordinates system\u001B[39;00m\n\u001B[32m    121\u001B[39m patched_map = \u001B[38;5;28mself\u001B[39m._patch_maps(det_maps)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Do/HerdNet/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001B[39m, in \u001B[36mcontext_decorator.<locals>.decorate_context\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    117\u001B[39m \u001B[38;5;129m@functools\u001B[39m.wraps(func)\n\u001B[32m    118\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdecorate_context\u001B[39m(*args, **kwargs):\n\u001B[32m    119\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[32m--> \u001B[39m\u001B[32m120\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Do/HerdNet/animaloc/eval/stitchers.py:235\u001B[39m, in \u001B[36mHerdNetStitcher._inference\u001B[39m\u001B[34m(self, patches)\u001B[39m\n\u001B[32m    233\u001B[39m heatmap = outputs[\u001B[32m0\u001B[39m]\n\u001B[32m    234\u001B[39m scale_factor = \u001B[32m16\u001B[39m\n\u001B[32m--> \u001B[39m\u001B[32m235\u001B[39m clsmap = F.interpolate(\u001B[43moutputs\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m]\u001B[49m, scale_factor=scale_factor, mode=\u001B[33m'\u001B[39m\u001B[33mnearest\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m    236\u001B[39m \u001B[38;5;66;03m# cat\u001B[39;00m\n\u001B[32m    237\u001B[39m outmaps = torch.cat([heatmap, clsmap], dim=\u001B[32m1\u001B[39m)\n",
      "\u001B[31mIndexError\u001B[39m: index 1 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Metrics"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T11:58:45.107414Z",
     "start_time": "2025-09-14T11:58:45.084362Z"
    }
   },
   "source": [
    "# Get detailed evaluation results\n",
    "if hasattr(evaluator, 'results'):\n",
    "    results = evaluator.results\n",
    "    print(\"\\n=== DETAILED RESULTS ===\")\n",
    "    print(results.head() if hasattr(results, 'head') else results)\n",
    "else:\n",
    "    print(\"\\nDetailed results not available\")\n",
    "\n",
    "# Additional metrics if available\n",
    "try:\n",
    "    precision = evaluator.evaluate(returns='precision')\n",
    "    recall = evaluator.evaluate(returns='recall')\n",
    "    print(f\"\\nPrecision: {precision * 100:.2f}%\")\n",
    "    print(f\"Recall: {recall * 100:.2f}%\")\n",
    "except:\n",
    "    print(\"\\nAdditional metrics (precision/recall) not available\")"
   ],
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "No metrics have been stored, please use the evaluate method first.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAssertionError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[11]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# Get detailed evaluation results\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28;43mhasattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mevaluator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mresults\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m)\u001B[49m:\n\u001B[32m      3\u001B[39m     results = evaluator.results\n\u001B[32m      4\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m=== DETAILED RESULTS ===\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Do/HerdNet/animaloc/eval/evaluators.py:269\u001B[39m, in \u001B[36mEvaluator.results\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    264\u001B[39m \u001B[38;5;129m@property\u001B[39m\n\u001B[32m    265\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mresults\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> pandas.DataFrame:\n\u001B[32m    266\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m''' Returns metrics by class (recall, precision, f1_score, mse, mae, and rmse) \u001B[39;00m\n\u001B[32m    267\u001B[39m \u001B[33;03m    in a pandas dataframe '''\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m269\u001B[39m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m._stored_metrics \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, \\\n\u001B[32m    270\u001B[39m         \u001B[33m'\u001B[39m\u001B[33mNo metrics have been stored, please use the evaluate method first.\u001B[39m\u001B[33m'\u001B[39m\n\u001B[32m    272\u001B[39m     metrics_cpy = \u001B[38;5;28mself\u001B[39m._stored_metrics.copy()\n\u001B[32m    274\u001B[39m     res = []\n",
      "\u001B[31mAssertionError\u001B[39m: No metrics have been stored, please use the evaluate method first."
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "herdnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
